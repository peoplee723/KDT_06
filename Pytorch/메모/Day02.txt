Day02

인공신경망(ANN)

-다층 퍼셉트론(Multi Layer Perceptron)
인간의 뉴런과 같은 원리로 학습해보면 어떨까?

입력-은닉-출력 층
입력: 자극 입력(피부로 자극을 받음)
은닉: 자극 분석-> 뉴런과 층(분석할 자극의 넓이, 강도 분석)
자극: 자극을 통한 반응 (분석한 자극을 바탕으로 반응)
and, or, xor, nand
=>퍼셉트론을 여러층 쌓아 올린 구조
가중치와 절편을 찾아야함-> 경사하강법
입력->은닉->자극 ==> 오차 반영하여 은닉의 가중치, 편향 값 수정

ax+b-> sigmoid-> 확률값 ==> 임계값이 패키지 마다 다름
# 학습횟수, 업데이트 횟수, 간격, .... =하이퍼파라미터 (과대적합<->과소적합 사이 균형점 찾기)
처음-> 임의의 가중치 부여-> 가중치를 늘리거나 줄여서 변화 확인(하이퍼 파라미터)
->최적의 가중치 찾기

++layer가 많으면 역전파시 기울기 소실/폭주 문제 발생
==>활성화 함수 변경, 가중치 초기화, 데이터 정규화...

-----------------------------------------------------------------------------------------------------
배치: 1회 학습량  (딥러닝 데이터는 너무 많아서 훈련 데이터를 쪼갬)
에포크: 훈련 횟수 (컴파일-훈련-예측까지 한바퀴)
Ex) 훈련데이터 1000개, 배치크기 20, 에포크 10-> 한번의 훈련에 가중치 50번 업데이트
=>10에포크이므로 총 500번 업데이트

Torch를 통한 학습 과정
[1] 데이터 준비 (전처리, 분리)
	- 가중치(w), 절편(b) 초기화 (zeros+ requires_grad=True)
	- 최적화도구 선택 (optim.기법이름([w,b], lr=0.01(간격 기본값))
[2] 학습
	- 에포크 설정 및 학습 반복
		- >cost 계산(MSE....) 및 오차 반영하여 가중치 개선
						(가중치 초기화 후 비용함수 미분->반영)
	- 학습 후 텐서 값 확인(.item)
	- 예측==> 최종 파라미터 값을 통해 예측
[3]테스트
	- 오차 반영 못하도록 바꿔야함

다중 선형-> 피쳐가 늘어남에 따라 가중치의 개수도 늘어나야함
모델 선언-> model=nn.모델명(피쳐,결과의 차원)







###데이터 전처리 필수!!!!, 모델을 단순할 수록 좋음