{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 전처리하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[1] 모듈, 데이터 불러오기<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Korpora import Korpora\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Korpora 는 다른 분들이 연구 목적으로 공유해주신 말뭉치들을\n",
      "    손쉽게 다운로드, 사용할 수 있는 기능만을 제공합니다.\n",
      "\n",
      "    말뭉치들을 공유해 주신 분들에게 감사드리며, 각 말뭉치 별 설명과 라이센스를 공유 드립니다.\n",
      "    해당 말뭉치에 대해 자세히 알고 싶으신 분은 아래의 description 을 참고,\n",
      "    해당 말뭉치를 연구/상용의 목적으로 이용하실 때에는 아래의 라이센스를 참고해 주시기 바랍니다.\n",
      "\n",
      "    # Description\n",
      "    Author : e9t@github\n",
      "    Repository : https://github.com/e9t/nsmc\n",
      "    References : www.lucypark.kr/docs/2015-pyconkr/#39\n",
      "\n",
      "    Naver sentiment movie corpus v1.0\n",
      "    This is a movie review dataset in the Korean language.\n",
      "    Reviews were scraped from Naver Movies.\n",
      "\n",
      "    The dataset construction is based on the method noted in\n",
      "    [Large movie review dataset][^1] from Maas et al., 2011.\n",
      "\n",
      "    [^1]: http://ai.stanford.edu/~amaas/data/sentiment/\n",
      "\n",
      "    # License\n",
      "    CC0 1.0 Universal (CC0 1.0) Public Domain Dedication\n",
      "    Details in https://creativecommons.org/publicdomain/zero/1.0/\n",
      "\n",
      "[Korpora] Corpus `nsmc` is already installed at C:\\Users\\KDP-25\\Korpora\\nsmc\\ratings_train.txt\n",
      "[Korpora] Corpus `nsmc` is already installed at C:\\Users\\KDP-25\\Korpora\\nsmc\\ratings_test.txt\n"
     ]
    }
   ],
   "source": [
    "# 데이터 불러오기\n",
    "nsmc= Korpora.load('nsmc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test 데이터셋만 전처리\n",
    "Data= nsmc.test\n",
    "DataDF=pd.DataFrame(Data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[2] 텍스트 데이터 전처리 <hr>\n",
    "- 토큰화/정제 (불용어, 구두점, 오타 등 처리)\n",
    "- 단어사전 생성\n",
    "- 문장 수치화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 단어 사전을 생성시 활용할 데이터셋\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, feature, label):\n",
    "        super().__init__()\n",
    "        self.feature= feature\n",
    "        self.label= label\n",
    "        self.length=feature.shape[0]\n",
    "\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.feature[index], self.label[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DS 인스턴스 생성\n",
    "nsmcDS=TextDataset(DataDF['text'], DataDF['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "굳 ㅋ 1\n"
     ]
    }
   ],
   "source": [
    "for f, l in nsmcDS:\n",
    "    print(f,l)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import *\n",
    "import string\n",
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 구두점, 특수문자 추출\n",
    "pun=list(string.punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 토큰화 인스턴스 생성\n",
    "okt= Okt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens=okt.pos(DataDF['text'][0], stem=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('굳다', 'Adjective'), ('ㅋ', 'KoreanParticle')]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 토큰화를 통한 단어사전 생성 ->Counter와 리스트 컴프리헨션을 통해\n",
    "# - 행마다 데이터를 추출하여 토큰 반환 및 Counter에 저장\n",
    "def make_vocab(dataset):\n",
    "    '''\n",
    "    DF 형태일 경우 인덱스 초기화하기!\n",
    "    '''\n",
    "    counter=Counter()\n",
    "    for text,label in dataset:\n",
    "        # 한글빼고 다지우기\n",
    "        text=re.sub('[^ㄱ-ㅎ가-힣]+',' ',text)\n",
    "        # 형태소 분석 (stem-> 어근으로 통일, norm-> 정규화)\n",
    "        tokens=okt.pos(text,norm=True,stem=True)\n",
    "        #불용어, 구두점, 특수문자 제거\n",
    "\n",
    "\n",
    "        # 형태소 단어 counter에 저장\n",
    "        for t in tokens:\n",
    "            counter.update(t)\n",
    "    # 단어 사전에 저장\n",
    "    vocab={'<PAD>':0, '<UNK>':1}\n",
    "    vocab.update({word: i+2 for i, word in enumerate(counter.items())})\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 토큰화를 통한 단어사전 생성 ->CountVectorize를 통해\n",
    "# - 행마다 데이터를 추출하여 리스트에 저장 -> 단어 사전 생성성\n",
    "def make_vocab(dataset):\n",
    "    tokenlists=[]\n",
    "    counter=Counter()\n",
    "    for text,label in dataset:\n",
    "        # 한글빼고 다지우기\n",
    "        text=re.sub('[^ㄱ-ㅎ가-힣]+',' ',text)\n",
    "        # 형태소 분석 (stem-> 어근으로 통일, norm-> 정규화)\n",
    "        tokens=okt.morphs(text)\n",
    "        #불용어, 구두점, 특수문자 제거\n",
    "\n",
    "\n",
    "        # 형태소 단어 tokenlists에 저장\n",
    "        tokenlists.append(tokens)\n",
    "    # 단어 사전 생성\n",
    "    cv=CountVectorizer()\n",
    "    cv.fit_transform(tokenlists)\n",
    "    vocab=cv.vocabulary_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 단어사전을 통해 문장 벡터화 진행\n",
    "# 받은 텍스트를 같은 방법으로 토큰화 -> 만들어진 단어사전과 대조하여 수치화\n",
    "def vectorize(vocab, text, tokenizer):\n",
    "    vector_list=[]\n",
    "    for t in text:\n",
    "        token_lists=tokenizer.pos(t)\n",
    "        vector_token=[vocab[token] if token in vocab else vocab['<UNK>'] for token in token_lists]\n",
    "        vector_list.append(vector_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 수치화 패딩\n",
    "def vectorize(tokenlist,padding=20):\n",
    "    ### 단어 사전 생성 및 변환\n",
    "    cv=CountVectorizer()\n",
    "    cv.fit_transform(tokenlist)\n",
    "    vo=cv.vocabulary_\n",
    "    vectorlist=tokenlist.copy()\n",
    "    for idx,sen in enumerate(vectorlist):\n",
    "        senlist=sen.split(' ')[1:]\n",
    "        length=len(senlist)\n",
    "        if length<padding:\n",
    "            for ind,st in enumerate(senlist):\n",
    "                senlist[ind]=vo[st]\n",
    "            vectorlist[idx]=senlist+([0]*(padding-length))\n",
    "        else:\n",
    "            for ind,st in enumerate(senlist):\n",
    "                senlist[ind]=vo[st]\n",
    "            vectorlist[idx]=senlist[:padding]\n",
    "\n",
    "    return vectorlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "50000",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\KDP-25\\anaconda3\\envs\\NLP\\lib\\site-packages\\pandas\\core\\indexes\\range.py:345\u001b[0m, in \u001b[0;36mRangeIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    344\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 345\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_range\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    346\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "\u001b[1;31mValueError\u001b[0m: 50000 is not in range",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m tokenlist\u001b[38;5;241m=\u001b[39m\u001b[43mgenerateToken\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnsmcDS\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[11], line 3\u001b[0m, in \u001b[0;36mgenerateToken\u001b[1;34m(dataset)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerateToken\u001b[39m(dataset):\n\u001b[0;32m      2\u001b[0m     tokenlists\u001b[38;5;241m=\u001b[39m[]\n\u001b[1;32m----> 3\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m text,label \u001b[38;5;129;01min\u001b[39;00m dataset:\n\u001b[0;32m      4\u001b[0m         \u001b[38;5;66;03m# 한글빼고 다지우기\u001b[39;00m\n\u001b[0;32m      5\u001b[0m         text\u001b[38;5;241m=\u001b[39mre\u001b[38;5;241m.\u001b[39msub(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m[^ㄱ-ㅎ가-힣]+\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m,text)\n\u001b[0;32m      6\u001b[0m         tokenlist\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "Cell \u001b[1;32mIn[6], line 15\u001b[0m, in \u001b[0;36mTextDataset.__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, index):\n\u001b[1;32m---> 15\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeature\u001b[49m\u001b[43m[\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m]\u001b[49m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabel[index]\n",
      "File \u001b[1;32mc:\\Users\\KDP-25\\anaconda3\\envs\\NLP\\lib\\site-packages\\pandas\\core\\series.py:1007\u001b[0m, in \u001b[0;36mSeries.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   1004\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[key]\n\u001b[0;32m   1006\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m key_is_scalar:\n\u001b[1;32m-> 1007\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1009\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_hashable(key):\n\u001b[0;32m   1010\u001b[0m     \u001b[38;5;66;03m# Otherwise index.get_value will raise InvalidIndexError\u001b[39;00m\n\u001b[0;32m   1011\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1012\u001b[0m         \u001b[38;5;66;03m# For labels that don't resolve as scalars like tuples and frozensets\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\KDP-25\\anaconda3\\envs\\NLP\\lib\\site-packages\\pandas\\core\\series.py:1116\u001b[0m, in \u001b[0;36mSeries._get_value\u001b[1;34m(self, label, takeable)\u001b[0m\n\u001b[0;32m   1113\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[label]\n\u001b[0;32m   1115\u001b[0m \u001b[38;5;66;03m# Similar to Index.get_value, but we do not fall back to positional\u001b[39;00m\n\u001b[1;32m-> 1116\u001b[0m loc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1118\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(loc):\n\u001b[0;32m   1119\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[loc]\n",
      "File \u001b[1;32mc:\\Users\\KDP-25\\anaconda3\\envs\\NLP\\lib\\site-packages\\pandas\\core\\indexes\\range.py:347\u001b[0m, in \u001b[0;36mRangeIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    345\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_range\u001b[38;5;241m.\u001b[39mindex(new_key)\n\u001b[0;32m    346\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m--> 347\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m    348\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Hashable):\n\u001b[0;32m    349\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 50000"
     ]
    }
   ],
   "source": [
    "# 패딩\n",
    "# 1. 선택한 길이가 문장길이보다 긴 경우 -> 문장 끝에 0추가\n",
    "# 2. 선택한 길이가 문장길이보다 짧은 경우 -> 패딩길이만큼 슬라이싱\n",
    "# ==> 일반적으로 한글은 중요한 정보가 앞쪽에 있다. -> 뒤쪽에 패딩추가 또는 슬라이싱\n",
    "def padding(length, textList):\n",
    "    pad_texts=[]\n",
    "    for text in textList:\n",
    "            # 선택 길이> 문장길이 일때\n",
    "        if length>len(text):\n",
    "                                            # 남은 텍스트 길이만큼 0으로 채우기\n",
    "            text=text+[0]*(length-text)\n",
    "        else: # 선택 길이< 문장 길이 일때\n",
    "            text=text[:length]              # 선택 길이만큼 슬라이싱\n",
    "        pad_texts.append(text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
