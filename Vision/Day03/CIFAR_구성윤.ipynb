{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CIFAR_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torchinfo import summary\n",
    "\n",
    "from torchinfo import summary\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 불러오기\n",
    "\n",
    "def unpickle(file):\n",
    "    import pickle\n",
    "    with open(file, 'rb') as fo:\n",
    "        dict = pickle.load(fo, encoding='bytes')\n",
    "    return dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature=pd.DataFrame()\n",
    "label=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "test=unpickle('../data/cifar-10-batches-py/test_batch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'> (10000, 3072)\n",
      "10000\n",
      "<class 'numpy.ndarray'> (10000, 3072)\n",
      "10000\n",
      "<class 'numpy.ndarray'> (10000, 3072)\n",
      "10000\n",
      "<class 'numpy.ndarray'> (10000, 3072)\n",
      "10000\n",
      "<class 'numpy.ndarray'> (10000, 3072)\n",
      "10000\n"
     ]
    }
   ],
   "source": [
    "for i in range(1,6):\n",
    "    file_name='../data/cifar-10-batches-py/data_batch_'+str(i)\n",
    "    data=unpickle(file_name)\n",
    "    print(type(data[b'data']), data[b'data'].shape)\n",
    "    print(len(data[b'labels']))\n",
    "    feature=pd.concat([feature, pd.DataFrame(data[b'data'])])\n",
    "    label.extend(data[b'labels'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature= feature.to_numpy().reshape(-1,3,32,32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((50000, 3, 32, 32), 50000)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature.shape, len(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[ 59,  43,  50, ..., 158, 152, 148],\n",
       "         [ 16,   0,  18, ..., 123, 119, 122],\n",
       "         [ 25,  16,  49, ..., 118, 120, 109],\n",
       "         ...,\n",
       "         [208, 201, 198, ..., 160,  56,  53],\n",
       "         [180, 173, 186, ..., 184,  97,  83],\n",
       "         [177, 168, 179, ..., 216, 151, 123]],\n",
       "\n",
       "        [[ 62,  46,  48, ..., 132, 125, 124],\n",
       "         [ 20,   0,   8, ...,  88,  83,  87],\n",
       "         [ 24,   7,  27, ...,  84,  84,  73],\n",
       "         ...,\n",
       "         [170, 153, 161, ..., 133,  31,  34],\n",
       "         [139, 123, 144, ..., 148,  62,  53],\n",
       "         [144, 129, 142, ..., 184, 118,  92]],\n",
       "\n",
       "        [[ 63,  45,  43, ..., 108, 102, 103],\n",
       "         [ 20,   0,   0, ...,  55,  50,  57],\n",
       "         [ 21,   0,   8, ...,  50,  50,  42],\n",
       "         ...,\n",
       "         [ 96,  34,  26, ...,  70,   7,  20],\n",
       "         [ 96,  42,  30, ...,  94,  34,  34],\n",
       "         [116,  94,  87, ..., 140,  84,  72]]],\n",
       "\n",
       "\n",
       "       [[[154, 126, 105, ...,  91,  87,  79],\n",
       "         [140, 145, 125, ...,  96,  77,  71],\n",
       "         [140, 139, 115, ...,  79,  68,  67],\n",
       "         ...,\n",
       "         [175, 156, 154, ...,  42,  61,  93],\n",
       "         [165, 156, 159, ..., 103, 123, 131],\n",
       "         [163, 158, 163, ..., 143, 143, 143]],\n",
       "\n",
       "        [[177, 137, 104, ...,  95,  90,  81],\n",
       "         [160, 153, 125, ...,  99,  80,  73],\n",
       "         [155, 146, 115, ...,  82,  70,  69],\n",
       "         ...,\n",
       "         [167, 154, 160, ...,  34,  53,  83],\n",
       "         [154, 152, 161, ...,  93, 114, 121],\n",
       "         [148, 148, 156, ..., 133, 134, 133]],\n",
       "\n",
       "        [[187, 136,  95, ...,  71,  71,  70],\n",
       "         [169, 154, 118, ...,  78,  62,  61],\n",
       "         [164, 149, 112, ...,  64,  55,  55],\n",
       "         ...,\n",
       "         [166, 160, 170, ...,  36,  57,  91],\n",
       "         [128, 130, 142, ...,  96, 120, 131],\n",
       "         [120, 122, 133, ..., 139, 142, 144]]],\n",
       "\n",
       "\n",
       "       [[[255, 253, 253, ..., 253, 253, 253],\n",
       "         [255, 255, 255, ..., 255, 255, 255],\n",
       "         [255, 254, 254, ..., 254, 254, 254],\n",
       "         ...,\n",
       "         [113, 111, 105, ...,  72,  72,  72],\n",
       "         [111, 104,  99, ...,  68,  70,  78],\n",
       "         [106,  99,  95, ...,  78,  79,  80]],\n",
       "\n",
       "        [[255, 253, 253, ..., 253, 253, 253],\n",
       "         [255, 255, 255, ..., 255, 255, 255],\n",
       "         [255, 254, 254, ..., 254, 254, 254],\n",
       "         ...,\n",
       "         [120, 118, 112, ...,  81,  80,  80],\n",
       "         [118, 111, 106, ...,  75,  76,  84],\n",
       "         [113, 106, 102, ...,  85,  85,  86]],\n",
       "\n",
       "        [[255, 253, 253, ..., 253, 253, 253],\n",
       "         [255, 255, 255, ..., 255, 255, 255],\n",
       "         [255, 254, 254, ..., 254, 254, 254],\n",
       "         ...,\n",
       "         [112, 111, 106, ...,  80,  79,  79],\n",
       "         [110, 104,  98, ...,  73,  75,  82],\n",
       "         [105,  98,  94, ...,  83,  83,  84]]],\n",
       "\n",
       "\n",
       "       ...,\n",
       "\n",
       "\n",
       "       [[[ 35,  40,  42, ...,  99,  79,  89],\n",
       "         [ 57,  44,  50, ..., 156, 141, 116],\n",
       "         [ 98,  64,  69, ..., 188, 119,  61],\n",
       "         ...,\n",
       "         [ 73,  53,  54, ...,  17,  21,  33],\n",
       "         [ 61,  55,  57, ...,  24,  17,   7],\n",
       "         [ 44,  46,  49, ...,  27,  21,  12]],\n",
       "\n",
       "        [[178, 176, 176, ..., 177, 147, 148],\n",
       "         [182, 184, 183, ..., 182, 177, 149],\n",
       "         [197, 189, 192, ..., 195, 135,  79],\n",
       "         ...,\n",
       "         [ 79,  63,  68, ...,  40,  36,  48],\n",
       "         [ 68,  70,  79, ...,  48,  35,  23],\n",
       "         [ 56,  66,  77, ...,  52,  43,  31]],\n",
       "\n",
       "        [[235, 239, 241, ..., 219, 197, 189],\n",
       "         [234, 250, 240, ..., 200, 206, 175],\n",
       "         [237, 252, 245, ..., 206, 147,  90],\n",
       "         ...,\n",
       "         [ 77,  68,  80, ...,  64,  51,  49],\n",
       "         [ 75,  86, 103, ...,  72,  53,  32],\n",
       "         [ 73,  88, 105, ...,  77,  66,  50]]],\n",
       "\n",
       "\n",
       "       [[[189, 186, 185, ..., 175, 172, 169],\n",
       "         [194, 191, 190, ..., 173, 171, 167],\n",
       "         [208, 205, 204, ..., 175, 172, 169],\n",
       "         ...,\n",
       "         [207, 203, 203, ..., 135, 162, 168],\n",
       "         [198, 189, 180, ..., 178, 175, 175],\n",
       "         [198, 189, 178, ..., 195, 196, 195]],\n",
       "\n",
       "        [[211, 208, 207, ..., 195, 194, 194],\n",
       "         [210, 207, 206, ..., 192, 191, 190],\n",
       "         [219, 216, 215, ..., 191, 190, 191],\n",
       "         ...,\n",
       "         [199, 195, 196, ..., 132, 158, 163],\n",
       "         [190, 181, 172, ..., 171, 169, 169],\n",
       "         [189, 181, 170, ..., 184, 189, 190]],\n",
       "\n",
       "        [[240, 236, 235, ..., 224, 222, 220],\n",
       "         [239, 236, 235, ..., 220, 218, 216],\n",
       "         [244, 240, 239, ..., 217, 216, 215],\n",
       "         ...,\n",
       "         [181, 175, 173, ..., 127, 150, 151],\n",
       "         [170, 159, 147, ..., 160, 156, 154],\n",
       "         [173, 162, 149, ..., 169, 171, 171]]],\n",
       "\n",
       "\n",
       "       [[[229, 236, 234, ..., 217, 221, 222],\n",
       "         [222, 239, 233, ..., 223, 227, 210],\n",
       "         [213, 234, 231, ..., 220, 220, 202],\n",
       "         ...,\n",
       "         [150, 140, 132, ..., 224, 230, 241],\n",
       "         [137, 130, 125, ..., 181, 202, 212],\n",
       "         [122, 118, 120, ..., 179, 164, 163]],\n",
       "\n",
       "        [[229, 237, 236, ..., 219, 223, 223],\n",
       "         [221, 239, 234, ..., 223, 228, 211],\n",
       "         [206, 232, 233, ..., 220, 219, 203],\n",
       "         ...,\n",
       "         [143, 135, 127, ..., 222, 228, 241],\n",
       "         [132, 127, 121, ..., 180, 201, 211],\n",
       "         [119, 116, 116, ..., 177, 164, 163]],\n",
       "\n",
       "        [[239, 247, 247, ..., 233, 234, 233],\n",
       "         [229, 249, 246, ..., 236, 238, 220],\n",
       "         [211, 239, 244, ..., 232, 232, 215],\n",
       "         ...,\n",
       "         [135, 127, 120, ..., 218, 225, 238],\n",
       "         [126, 120, 115, ..., 178, 198, 207],\n",
       "         [114, 110, 111, ..., 173, 162, 161]]]], dtype=uint8)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature.shape, type(feature)\n",
    "feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "testF=test[b'data']\n",
    "targetF=test[b'labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Custom_Dataset(Dataset):\n",
    "    def __init__(self, featureDF, targetDF) -> None:\n",
    "        super().__init__()\n",
    "        self.featureDF= featureDF\n",
    "        self.targetDF= targetDF\n",
    "        self.n_rows= len(feature)\n",
    "\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.n_rows\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        featureTS= torch.FloatTensor(self.featureDF[index])\n",
    "        targetTS= torch.FloatTensor(self.targetDF[index])\n",
    "        return featureTS, targetTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 32, 32])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "featureDL= DataLoader(torch.FloatTensor(feature))\n",
    "targetDL=DataLoader(label, batch_size=1)\n",
    "# 채널이 앞에 있네? -> conv2d 위해 shape 자동 변경해줌\n",
    "# 이미지 시각화 -> shape 변경이 필요함 (행,열,채널)\n",
    "for f in featureDL:\n",
    "    plt.imshow(f.squeeze().permute(1,2,0))\n",
    "    print(f.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImgMCF2(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        # 특징 추출 \n",
    "        self.cnn_layer=nn.Sequential(\n",
    "            nn.Conv2d(in_channels=3, out_channels=10,\n",
    "                                    kernel_size=3),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, stride=2)\n",
    "        )\n",
    "        self.cnn_layer2=nn.Sequential(\n",
    "            nn.Conv2d(10,30,3),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, stride=2)\n",
    "        )\n",
    "        # 학습\n",
    "        \n",
    "        self.hd_layer=nn.Linear(30*6*6,50)\n",
    "        self.out_layer=nn.Linear(50,10)\n",
    "\n",
    "    # 순방향 학습 메서드\n",
    "    def forward(self, input):\n",
    "        self.cnn_layer(input)\n",
    "\n",
    "        out=out.view(out.shape[0], -1)\n",
    "\n",
    "        out=self.hd_layer(out)\n",
    "        out=F.relu(out)\n",
    "\n",
    "        out=self.out_layer(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 3, 32, 32)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature=feature.to_numpy().reshape(-1,3,32,32)\n",
    "trainDS=Custom_Dataset(feature, label)\n",
    "testDS=Custom_Dataset(testF, targetF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE=50\n",
    "trainDL= DataLoader(trainDS, batch_size=BATCH_SIZE)\n",
    "testDL=DataLoader(testDS, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Module.parameters of ImgMCF2(\n",
       "  (cnn_layer): Sequential(\n",
       "    (0): Conv2d(3, 10, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (1): ReLU()\n",
       "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (cnn_layer2): Sequential(\n",
       "    (0): Conv2d(10, 30, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (1): ReLU()\n",
       "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (hd_layer): Linear(in_features=1080, out_features=50, bias=True)\n",
       "  (out_layer): Linear(in_features=50, out_features=10, bias=True)\n",
       ")>"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 파라미터 선언\n",
    "LR=0.001\n",
    "model=ImgMCF2()\n",
    "DEVICE= 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model.to(DEVICE)\n",
    "criterion= nn.CrossEntropyLoss()\n",
    "optimizer=optim.Adam(model.parameters(), lr=LR)\n",
    "model.parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "stack expects each tensor to be equal size, but got [6] at entry 0 and [9] at entry 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[79], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m f, t \u001b[38;5;129;01min\u001b[39;00m trainDL:\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;28mprint\u001b[39m(f, t)\n",
      "File \u001b[1;32mc:\\Users\\KDP-25\\anaconda3\\envs\\TORCH_CV_38\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Users\\KDP-25\\anaconda3\\envs\\TORCH_CV_38\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:673\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    671\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    672\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 673\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    674\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    675\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\KDP-25\\anaconda3\\envs\\TORCH_CV_38\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:55\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[1;32m---> 55\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\KDP-25\\anaconda3\\envs\\TORCH_CV_38\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:317\u001b[0m, in \u001b[0;36mdefault_collate\u001b[1;34m(batch)\u001b[0m\n\u001b[0;32m    256\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdefault_collate\u001b[39m(batch):\n\u001b[0;32m    257\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    258\u001b[0m \u001b[38;5;124;03m    Take in a batch of data and put the elements within the batch into a tensor with an additional outer dimension - batch size.\u001b[39;00m\n\u001b[0;32m    259\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    315\u001b[0m \u001b[38;5;124;03m        >>> default_collate(batch)  # Handle `CustomType` automatically\u001b[39;00m\n\u001b[0;32m    316\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 317\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdefault_collate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\KDP-25\\anaconda3\\envs\\TORCH_CV_38\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:174\u001b[0m, in \u001b[0;36mcollate\u001b[1;34m(batch, collate_fn_map)\u001b[0m\n\u001b[0;32m    171\u001b[0m transposed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mbatch))  \u001b[38;5;66;03m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[0;32m    173\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m--> 174\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [collate(samples, collate_fn_map\u001b[38;5;241m=\u001b[39mcollate_fn_map) \u001b[38;5;28;01mfor\u001b[39;00m samples \u001b[38;5;129;01min\u001b[39;00m transposed]  \u001b[38;5;66;03m# Backwards compatibility.\u001b[39;00m\n\u001b[0;32m    175\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    176\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\KDP-25\\anaconda3\\envs\\TORCH_CV_38\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:174\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    171\u001b[0m transposed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mbatch))  \u001b[38;5;66;03m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[0;32m    173\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m--> 174\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43msamples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m samples \u001b[38;5;129;01min\u001b[39;00m transposed]  \u001b[38;5;66;03m# Backwards compatibility.\u001b[39;00m\n\u001b[0;32m    175\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    176\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\KDP-25\\anaconda3\\envs\\TORCH_CV_38\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:142\u001b[0m, in \u001b[0;36mcollate\u001b[1;34m(batch, collate_fn_map)\u001b[0m\n\u001b[0;32m    140\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m collate_fn_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    141\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m elem_type \u001b[38;5;129;01min\u001b[39;00m collate_fn_map:\n\u001b[1;32m--> 142\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcollate_fn_map\u001b[49m\u001b[43m[\u001b[49m\u001b[43melem_type\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    144\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m collate_type \u001b[38;5;129;01min\u001b[39;00m collate_fn_map:\n\u001b[0;32m    145\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, collate_type):\n",
      "File \u001b[1;32mc:\\Users\\KDP-25\\anaconda3\\envs\\TORCH_CV_38\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:214\u001b[0m, in \u001b[0;36mcollate_tensor_fn\u001b[1;34m(batch, collate_fn_map)\u001b[0m\n\u001b[0;32m    212\u001b[0m     storage \u001b[38;5;241m=\u001b[39m elem\u001b[38;5;241m.\u001b[39m_typed_storage()\u001b[38;5;241m.\u001b[39m_new_shared(numel, device\u001b[38;5;241m=\u001b[39melem\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m    213\u001b[0m     out \u001b[38;5;241m=\u001b[39m elem\u001b[38;5;241m.\u001b[39mnew(storage)\u001b[38;5;241m.\u001b[39mresize_(\u001b[38;5;28mlen\u001b[39m(batch), \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mlist\u001b[39m(elem\u001b[38;5;241m.\u001b[39msize()))\n\u001b[1;32m--> 214\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: stack expects each tensor to be equal size, but got [6] at entry 0 and [9] at entry 1"
     ]
    }
   ],
   "source": [
    "for f, t in trainDL:\n",
    "    print(f, t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCH=5\n",
    "cnt=0\n",
    "loss_list, iteration_list, accuaracy_list= [],[],[]\n",
    "predictions_list, labels_list=[],[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 12288 is out of bounds for axis 0 with size 10000",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[119], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m ep \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(EPOCH):\n\u001b[1;32m----> 2\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m images, labels \u001b[38;5;129;01min\u001b[39;00m trainDL:\n\u001b[0;32m      3\u001b[0m         images, labels\u001b[38;5;241m=\u001b[39m images\u001b[38;5;241m.\u001b[39mto(DEVICE), labels\u001b[38;5;241m.\u001b[39mto(DEVICE)\n\u001b[0;32m      5\u001b[0m         train\u001b[38;5;241m=\u001b[39mVariable(images\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m100\u001b[39m,\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m28\u001b[39m,\u001b[38;5;241m28\u001b[39m))\n",
      "File \u001b[1;32mc:\\Users\\KDP-25\\anaconda3\\envs\\TORCH_CV_38\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Users\\KDP-25\\anaconda3\\envs\\TORCH_CV_38\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:673\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    671\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    672\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 673\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    674\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    675\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\KDP-25\\anaconda3\\envs\\TORCH_CV_38\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\KDP-25\\anaconda3\\envs\\TORCH_CV_38\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[1;32mIn[112], line 13\u001b[0m, in \u001b[0;36mCustom_Dataset.__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, index):\n\u001b[1;32m---> 13\u001b[0m     featureTS\u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mFloatTensor(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeatureDF\u001b[49m\u001b[43m[\u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m3072\u001b[39;49m\u001b[43m]\u001b[49m)\n\u001b[0;32m     14\u001b[0m     targetTS\u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mFloatTensor(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtargetDF[index])\n\u001b[0;32m     15\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m featureTS, targetTS\n",
      "\u001b[1;31mIndexError\u001b[0m: index 12288 is out of bounds for axis 0 with size 10000"
     ]
    }
   ],
   "source": [
    "for ep in range(EPOCH):\n",
    "    for images, labels in trainDL:\n",
    "\n",
    "        train=Variable(images.view(100,1,28,28))\n",
    "        labels=Variable(labels)\n",
    "\n",
    "        outputs= model(train)\n",
    "        loss= criterion(outputs, labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        cnt+=1\n",
    "        if ~(cnt%50):\n",
    "            total, correct=0,0\n",
    "            for images, labels in testDL:\n",
    "                images, labels= images.to(DEVICE), labels.to(DEVICE)\n",
    "                labels_list.append(labels)\n",
    "                test=Variable(images.view(100,1,28,28))\n",
    "                outputs=model(test)\n",
    "                predictions=torch.max(outputs, 1)[1].to(DEVICE)\n",
    "                predictions_list.append(predictions)\n",
    "                correct+=(predictions==labels).sum()\n",
    "                total+=len(labels)\n",
    "            \n",
    "            accuaracy=correct*100/total\n",
    "            loss_list.append(loss.data)\n",
    "            iteration_list.append(cnt)\n",
    "            accuaracy_list.append(accuaracy)\n",
    "        if ~(cnt%500):\n",
    "            print(f'Iteration: {cnt}, Loss: {loss.data}, Accuracy: {accuaracy}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TORCH_CV_38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
